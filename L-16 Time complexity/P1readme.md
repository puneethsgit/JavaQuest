# Time Complexity Graphs

This repository contains visualizations of common time complexities and their growth with increasing input size. Understanding the time complexity of algorithms is essential for optimizing code performance and choosing the right algorithm for a given problem.

## Time Complexities Covered

- **O(1)** - Constant Time
- **O(log n)** - Logarithmic Time
- **O(n)** - Linear Time
- **O(n log n)** - Linearithmic Time
- **O(n²)** - Quadratic Time
- **O(n³)** - Cubic Time
- **O(2^n)** - Exponential Time
- **O(n!)** - Factorial Time

## Time Complexity Graph

The following graph compares the growth of these time complexities as the input size \( n \) increases.

![Time Complexity Graph](https://drive.google.com/uc?export=view&id=1PtUXwFmPdotJaXyjXuJ12aRPHqs0Q7QZ)

## Description of the Graph

- **O(1)**: Constant time complexity, which means the algorithm runs in the same time regardless of the input size.
- **O(log n)**: Logarithmic time complexity, typically seen in algorithms that halve the input size in each step (e.g., Binary Search).
- **O(n)**: Linear time complexity, where the algorithm's time increases directly with the input size.
- **O(n log n)**: Linearithmic time complexity, commonly found in efficient sorting algorithms like Merge Sort and Quick Sort.
- **O(n²)**: Quadratic time complexity, seen in algorithms with nested loops over the input (e.g., Bubble Sort).
- **O(n³)**: Cubic time complexity, where the algorithm's time grows even faster with input size.
- **O(2^n)**: Exponential time complexity, typically seen in brute-force solutions to problems like the Traveling Salesman Problem.
- **O(n!)**: Factorial time complexity, which grows extremely fast, as seen in problems that require checking all possible permutations.

